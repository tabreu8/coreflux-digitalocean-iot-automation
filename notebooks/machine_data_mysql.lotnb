[
  {
    "kind": 1,
    "language": "markdown",
    "value": "# Coreflux MQTT Broker with MySQL - Complete IoT Automation Tutorial\n\n### ‚¨ÖÔ∏è [Return to Main Notebook](../digitalocean_lot_examples.lotnb)\n- üîó  [MySQL Article on DigitalOcean](www.google.com)\n\n## Step 1: Database Integration Setup\n\nThe first step in our IoT automation pipeline is establishing a connection between our MQTT broker and the MySQL database. Routes in Coreflux define how processed data flows to external systems.\n\n### Understanding Routes\nA Route is a pathway that defines how your processed IoT data gets stored in external systems like databases. Think of it as a bridge between your real-time MQTT data and your persistent storage solution. It automatically creates the necessary tables in the database and stores data automatically when it is updated in the model.\n\n### Key Components:\n\n- **Route Name**: A unique identifier for your database connection\n- **Type**: Specifies the database type (MONGODB, POSTGRESQL, MYSQL, etc.)\n- **Connection String**: Your database credentials and connection details\n- **Database**: The target database name for storing your IoT data\n\n### üîß Configure Your MySQL Route\nReplace the connection parameters below with your actual MySQL connection details from DigitalOcean:"
  },
  {
    "kind": 2,
    "language": "lot",
    "value": "DEFINE ROUTE MySQL_Log WITH TYPE MYSQL\n    ADD SQL_CONFIG\n        WITH SERVER \"db-mysql.db.onmyserver.com\"\n        WITH PORT 25060\n        WITH DATABASE \"defaultdb\"\n        WITH USERNAME \"doadmin\"\n        WITH PASSWORD \"AVNS_pass_placeholder\"\n        WITH USE_SSL TRUE\n        WITH TRUST_SERVER_CERTIFICATE FALSE"
  },
  {
    "kind": 1,
    "language": "markdown",
    "value": "## Step 2: Data Simulation for Testing\n\nSince we're building a demo system without physical IoT devices, we'll create Actions to simulate real sensor data. **Actions** are executable logic blocks that can be triggered by timers, topic updates, or system events.\nWe will be simulating two machines, so that we can show how we can apply a standard processing pipeline to multiple devices without any additional effort in the next steps of this tutorial.\n\n### Understanding Actions\nAn Action in LoT is like a scheduled or event triggered task that can:\n\n- Generate data at regular intervals or when triggered\n- Respond to MQTT topic changes\n- Perform calculations and publish results\n\n### Why Simulate Data?\n\n- Test your pipeline without physical hardware\n- Create predictable data patterns for development\n- Validate data transformation logic\n- Demonstrate system capabilities\n\n### üöÄ Data Generation Strategies\nWe provide two different approaches for data simulation. Choose the one below that best fits your testing needs.\n\n### üîÑ Running Your Data Simulator\n\n- **Choose one action** from the options above\n- **Execute the code block** in your LoT Notebook\n- **Monitor the sync status** - ensure it shows \"synced\" with your broker\n- **Verify data flow** using MQTT Explorer or other MQTT Client"
  },
  {
    "kind": 1,
    "language": "markdown",
    "value": "### Option 1: Incremental Data Generator (Recommended)\nThis approach creates more predictable machine behavior with incremental counters and reset logic.\n\n\n#### This action:\n- Initializes counters to 0 if topics are empty\n- Increments machine1 counter (0-10 range)\n- Increments station2 counter (0-60 range)\n- Resets counters when they reach maximum values\n- Runs every 10 seconds"
  },
  {
    "kind": 2,
    "language": "lot",
    "value": "DEFINE ACTION GenerateMachineData\nON EVERY 10 SECONDS DO\n    IF (GET TOPIC \"raw_data/machine1\" EQUALS EMPTY) THEN\n        PUBLISH TOPIC \"raw_data/machine1\" WITH 0\n        PUBLISH TOPIC \"raw_data/station2\" WITH 0\n    ELSE\n        IF (GET TOPIC \"raw_data/machine1\" >= 10) THEN\n            PUBLISH TOPIC \"raw_data/machine1\" WITH 0\n        ELSE\n            PUBLISH TOPIC \"raw_data/machine1\" WITH (GET TOPIC \"raw_data/machine1\" + 1)\n        IF (GET TOPIC \"raw_data/station2\" >= 60) THEN\n            PUBLISH TOPIC \"raw_data/station2\" WITH 0\n        ELSE\n            PUBLISH TOPIC \"raw_data/station2\" WITH (GET TOPIC \"raw_data/station2\" + 1)\n"
  },
  {
    "kind": 1,
    "language": "markdown",
    "value": "### Option 2: Random Data Generator\n\nThis approach generates completely random values for more varied testing.\n\n#### This action:\n- Generates random values for machine1 (0-10)\n- Generates random values for station2 (0-60)\n- Creates unpredictable data patterns\n- Useful for stress testing"
  },
  {
    "kind": 2,
    "language": "lot",
    "value": "DEFINE ACTION RANDOMIZEMachineData\nON EVERY 10 SECONDS DO\n    PUBLISH TOPIC \"raw_data/machine1\" WITH RANDOM BETWEEN 0 AND 10\n    PUBLISH TOPIC \"raw_data/station2\" WITH RANDOM BETWEEN 0 AND 60"
  },
  {
    "kind": 1,
    "language": "markdown",
    "value": "## Step 3: Data Transformation with Models\n\nNow we'll create a Model to transform raw sensor data into structured, meaningful information. Models are the heart of your data processing pipeline and implement the Unified Namespace (UNS) approach.\n\n### Understanding Models\nA **Model** in Coreflux:\n- Transforms raw IoT data into structured formats\n- Applies business logic and calculations\n- Creates a unified data schema across multiple devices\n- Publishes processed data to new MQTT topics\n- Stores results in databases for persistent storage\n\n### UNS (Unified Namespace) Approach\nThe UNS approach creates a single, consistent data structure that can be applied to multiple similar devices or sensors. Instead of writing separate logic for each device, you use wildcards (+) to create scalable solutions that applies automatically to all devices that follow the topic pattern.\n\n### üè≠ Industrial IoT Data Model\nOur model transforms raw energy readings into comprehensive machine production data.\n\n### üìä Model Field Breakdown\nLet's understand each field in our data transformation:\n\n| Field | Purpose | Logic |\n|-------|---------|--------|\n| `energy` | **Trigger Field** - Raw sensor input | Uses wildcard `+` to capture data from any matching topic |\n| `device_name` | **Device Identification** | Extracts device name from topic structure |\n| `energy_wh` | **Unit Conversion** | Converts energy to watt-hours (√ó1000) |\n| `production_status` | **Status Classification** | \"active\" if energy > 5, otherwise \"inactive\" |\n| `production_count` | **Production Tracking** | Incremental counter when machine is active |\n| `stoppage` | **Downtime Monitoring** | Flags when machine is inactive |\n| `maintenance_alert` | **Predictive Maintenance** | Alert when energy exceeds threshold |\n| `timestamp` | **Time Series Data** | UTC timestamp for data chronology |\n\n### üîÑ How the Wildcard Works\n\nThe wildcard `+` in `\"raw_data/+\"` means this model will automatically apply to:\n- `raw_data/machine1` ‚Üí Creates `MySQL/Simulator/Machine/machine1/Data`\n- `raw_data/station2` ‚Üí Creates `MySQL/Simulator/Machine/station2/Data`\n- Any future `raw_data/deviceX` ‚Üí Automatically processed\n\n### üíæ Database Storage\n\nThe `STORE IN \"MySQL_Log\"` directive:\n- Saves processed data to your MySQL cluster\n- Creates a collection named \"MachineProductionData\"\n- Maintains real-time synchronization\n- Preserves data for analytics and reporting"
  },
  {
    "kind": 2,
    "language": "lot",
    "value": "DEFINE MODEL MachineDataMySQL WITH TOPIC \"MySQL/Simulator/Machine/+/Data\"\n    ADD \"energy\" WITH TOPIC \"raw_data/+\" AS TRIGGER\n    ADD \"device_name\" WITH REPLACE \"+\" WITH TOPIC POSITION 2 IN \"+\"\n    ADD \"energy_wh\" WITH (energy * 1000)\n    ADD \"production_status\" WITH (IF energy > 5 THEN \"active\" ELSE \"inactive\")\n    ADD \"production_count\" WITH (IF production_status EQUALS \"active\" THEN (production_count + 1) ELSE 0)\n    ADD \"stoppage\" WITH (IF production_status EQUALS \"inactive\" THEN 1 ELSE 0)\n    ADD \"maintenance_alert\" WITH (IF energy > 50 THEN TRUE ELSE FALSE)\n    ADD \"timestamp\" WITH TIMESTAMP \"UTC\"\n    STORE IN \"MySQL_Log\"\n        WITH TABLE \"MachineProductionData\""
  },
  {
    "kind": 1,
    "language": "markdown",
    "value": "## Step 4: Testing and Verification\n\nAfter implementing your IoT automation pipeline, it's crucial to verify that everything works correctly.\n\n### üîç What to Check\n\n1. **MQTT Data Flow**\n   - Subscribe to `raw_data/+` to see simulated input data\n   - Subscribe to `MySQL/Simulator/Machine/+/Data` to see processed output\n   - Verify data appears every 10 seconds\n\n2. **Database Storage**\n   - Connect to MySQL using DBeaver\n   - Navigate to `coreflux_data` database\n   - Check `machineproductiondata` table for new documents\n\n3. **Data Transformation**\n   - Verify energy values are converted to watt-hours\n   - Check production_status logic (active/inactive)\n   - Confirm timestamps are in UTC format\n\n### üìä Expected Output Format\n\nYour processed data in MySQL should look like this:\n\n| id | energy | device_name | energy_wh | production_status | production_count | stoppage | maintenance_alert | timestamp |\n|----|--------|-------------|-----------|-------------------|------------------|----------|-------------------|-----------|\n| 3162 | 56 | station2 | 56,000 | active | 45 | 0 | [v] (or **True**) | 2025-07-02 17:36:57 |\n\n\n## üéâ Congratulations!\n\nYou've successfully built a complete IoT automation pipeline that:\n- ‚úÖ Simulates real-time sensor data\n- ‚úÖ Transforms raw data into meaningful insights\n- ‚úÖ Stores processed data in a scalable database\n- ‚úÖ Creates a foundation for advanced IoT applications\n\nYour system is now ready for real-world IoT devices and can scale to handle thousands of sensors across multiple locations. The UNS approach ensures that adding new devices requires minimal code changes, making your solution both maintainable and scalable.\n\n---\n\n## üîß Troubleshooting\n\n### Common Issues and Solutions\n\n**Action not generating data:**\n- Check broker connection status in LoT Notebook\n- Verify action is synced (not showing \"unsync\" status)\n- Use MQTT Explorer to monitor topic activity\n\n**Model not processing data:**\n- Ensure trigger topic matches your action's publish topics\n- Check for syntax errors in model definition\n- Verify wildcard patterns are correctly formatted\n\n**Database not receiving data:**\n- Confirm Database connection parameters are correct\n- Verify VPC network connectivity between droplet and database\n\n**Data format issues:**\n- Review field calculations and conditional logic\n- Test individual field transformations\n- Check timestamp format and timezone settings"
  }
]